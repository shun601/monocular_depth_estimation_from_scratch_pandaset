{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cee9fa6-0b22-47db-b970-22ae3b217b86",
   "metadata": {},
   "source": [
    "# 単眼深度推定モデルを学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc728c2-a249-4b7f-99f3-4966fce0c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1beef-ffe6-443b-8bef-8a1ad5935cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed204ba7-d45f-45f6-aa64-e13bdbf9d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_inds = [0, -1] # 隣接フレームの番号\n",
    "epochs = 1 #100\n",
    "lr = 0.0004\n",
    "batch_size = 8 #32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33003d41-9f92-4460-8629-f0a721f67be6",
   "metadata": {},
   "source": [
    "# ネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdd6ee-5b6f-484e-b1a7-330485bafbfc",
   "metadata": {},
   "source": [
    "学習するネットワークは現在のフレームの深度マップを推定するものと、フレーム間の姿勢の変化を推定するものの２つである。  \n",
    "ここではそれぞれdepth netとpose netと呼ぶ。\n",
    "- depth netは深度マップを推定するネットワーク\n",
    "- pose netは回転（X軸、Y軸、Z軸）と並進（X, Y, Z）の合計6個の数値を推定するネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9e2f1-665b-4277-9cbd-b2aad917b78d",
   "metadata": {},
   "source": [
    "## depth net\n",
    "ここではネットワークの詳細に関心はないたえ、`segmentation_models_pytorch`を使いU-Netを定義する。  \n",
    "最新の深度推定モデルは複数の解像度の深度マップを推定するのが一般的だが、ここではGPUのメモリを節約するために、１枚のみとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8bbf17-f196-4464-b4dd-59113267f8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.0943, -3.2927, -3.1944,  ...,  1.0344,  1.0198,  0.7016],\n",
       "          [-1.3239, -1.4857, -1.7132,  ..., -0.1457, -1.8472, -0.8712],\n",
       "          [ 0.4984, -2.9139, -1.1214,  ..., -1.4206, -2.2813, -3.1807],\n",
       "          ...,\n",
       "          [ 1.9982, -1.0578, -2.3860,  ..., -2.2841, -4.0713, -0.7282],\n",
       "          [-0.6151, -1.3243, -3.7924,  ..., -2.3052, -5.5659, -2.8069],\n",
       "          [-0.9539,  1.4243, -1.6765,  ..., -2.3851, -2.2154, -1.8759]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "depth_net = smp.Unet(\"efficientnet-b0\", in_channels=3, classes=1, activation=None)\n",
    "\n",
    "# 入出力確認\n",
    "depth_net(torch.zeros(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602f629-eb17-4f27-bc0c-a73b8ba4c002",
   "metadata": {},
   "source": [
    "## pose net\n",
    "pose netに関してもネットワークの詳細に関心がないため、`torchvision`の学習済みモデルを持ってきて、部分的にレイヤーを差し替える。\n",
    "現在フレームと隣接フレームの２枚を入力し、その２枚の間で発生した姿勢の変化量を推定する。  \n",
    "- 入力: 2枚の画像はチャンネルの次元で結合して合計で6チャンネルのテンソルとして入力を行う。\n",
    "- 出力： 出力は並進(X軸、Y軸、Z軸)と回転(X軸、Y軸、Z軸)の6DoFである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b456a6-fb3c-4a42-9f4a-268694766544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7779e-04,  1.3395e-04,  3.1361e-05,  2.5756e-04, -9.7372e-05,\n",
       "         -1.6779e-04]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch.nn import Linear, Conv2d\n",
    "\n",
    "pose_net = resnet18(pretrained=True)\n",
    "pose_net.conv1 = Conv2d(in_channels=3*2, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "pose_net.fc = Linear(in_features=512, out_features=6)\n",
    "\n",
    "# 初期はすべて0を出力したほうが都合がよいので、weightとbiasを0クリアしておく\n",
    "# reproj_loss_ += torch.randn(reproj_loss_.shape).to(device=reproj_loss_.device) * 1e-3\n",
    "pose_net.fc.weight.data = torch.randn(pose_net.fc.weight.shape) * 1e-5\n",
    "pose_net.fc.bias.data = torch.randn(pose_net.fc.bias.shape) * 1e-5\n",
    "\n",
    "# 入出力確認\n",
    "pose_net(torch.zeros(1, 3*2, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91511f1-ef0d-4e4c-9780-0cf7e31d8fe8",
   "metadata": {},
   "source": [
    "# Loss関数\n",
    "ここでは、Phtometric lossとSmoothness lossを定義する。  \n",
    "Photometric lossは推定した深度と姿勢変化からSourceの画像をTargetの画像に合わせて一致しているかどうかをl1とSSIMで測る。  \n",
    "また、[Monodepth2](https://arxiv.org/abs/1806.01260)で提案されたOcculusionや動体から生じる原理的に復元不可能な画素に対するLossの計算を回避するauto-maskingを導入した。  \n",
    "Smoothness lossはPhotometric lossの復元誤差が濃淡が平滑な領域で勾配を得にくいという問題を解決するために導入した。  \n",
    "このlossは近接ピクセルがおおよそ同じような深度を持っている（物体の境界以外は）という仮定のもと、それをLossとして与えるものである。  \n",
    "実装は様々あるが、ここではX,Y方向の差分のみから計算する比較的にシンプルなものを用いる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ea5fd3-5bd7-4897-9ce2-6e9c833af4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d01700d-7ee7-4d90-a914-bb25801e597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from camera import PinholeCamera\n",
    "from functools import lru_cache\n",
    "import torchgeometry as tgm\n",
    "\n",
    "\n",
    "class PhotometricLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, frame_inds, weights=[0.6, 0.4], automasking=False):\n",
    "        \"\"\"\n",
    "        Photometric loss\n",
    "        weights: l1とssimのlossに対する重みを指定する。デフォルトは[0.4, 0.6]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.frame_inds = frame_inds\n",
    "        self.l1_loss = torch.nn.L1Loss(reduction=\"none\")\n",
    "        self.ssim_loss = tgm.losses.SSIM(reduction='none', window_size=3)\n",
    "        self.weights = weights\n",
    "        self.automasking = automasking\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        image_target = y[\"rgb_0\"]\n",
    "        depth = y_pred[f\"depth_0\"]\n",
    "        intrinsic = y[\"intrinsic_0\"]\n",
    "\n",
    "        reproj_loss = []\n",
    "        image_warped = {}\n",
    "        for idx in self.frame_inds:\n",
    "            if idx == 0: # ターゲット同士は比較しない\n",
    "                continue\n",
    "            image_source = y[f\"rgb_{idx}\"]\n",
    "            extrinsic_src2tgt = y_pred[f\"extrinsic_{idx}\"]\n",
    "            image_warped_ = self.warp(image_source, depth, intrinsic, extrinsic_src2tgt)\n",
    "            l1_loss = self.l1_loss(image_warped_, image_target)\n",
    "            ssim_loss = self.ssim_loss(image_warped_, image_target)\n",
    "            reproj_loss_ = l1_loss * self.weights[0] + ssim_loss * self.weights[1]\n",
    "            reproj_loss_ = torch.mean(reproj_loss_, dim=1) # auto-maskingで扱いやすいようにチャンネルの次元を潰しておく\n",
    "            reproj_loss.append(reproj_loss_)\n",
    "            image_warped[idx] = image_warped_\n",
    "        \n",
    "        if self.automasking:\n",
    "            # auto-masking (https://arxiv.org/pdf/1806.01260.pdf) 何も変更を加えないSource画像を利用する。\n",
    "            for idx in self.frame_inds:\n",
    "                if idx == 0: # ターゲット同士は比較しない\n",
    "                    continue\n",
    "                image_source = y[f\"rgb_{idx}\"]\n",
    "                l1_loss = self.l1_loss(image_source, image_target)\n",
    "                ssim_loss = self.ssim_loss(image_source, image_target)\n",
    "                reproj_loss_ = l1_loss * self.weights[0] + ssim_loss * self.weights[1]\n",
    "                # 平坦な領域ではWarpされたものと何も変更を加えないものでLossが全くおなじになってしまう画素が生じる可能性があるので微小な乱数を加える\n",
    "                reproj_loss_ += torch.randn(reproj_loss_.shape).to(device=reproj_loss_.device) * 1e-3\n",
    "                reproj_loss_ = torch.mean(reproj_loss_, dim=1) # auto-maskingで扱いやすいようにチャンネルの次元を潰しておく\n",
    "                reproj_loss.append(reproj_loss_)\n",
    "\n",
    "            reproj_loss = torch.stack(reproj_loss, dim=1)\n",
    "            loss, min_inds = torch.min(reproj_loss, dim=1)\n",
    "            automask = (min_inds >= (reproj_loss.shape[1] // 2)).float()\n",
    "            loss = reproj_loss.mean()\n",
    "        else:\n",
    "            loss = torch.stack(reproj_loss, dim=1)\n",
    "            automask = torch.zeros_like(loss).squeeze(1) # dummy\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        return loss, image_warped, automask\n",
    "\n",
    "    def warp(self, image_source, depth, intrinsic, extrinsic_src2tgt):        \n",
    "        \"\"\" 推定された深度と姿勢からソースをターゲットに重ね合わせる \"\"\"\n",
    "        image_coords = self.create_image_coords(depth.shape)\n",
    "        image_coords = image_coords.to(depth.device)\n",
    "        world_coords = PinholeCamera.image2world(image_coords, intrinsic, extrinsic_src2tgt, depth, batch=True)        \n",
    "        # これまでノートとは異なりターゲットのカメラへの座標変換が終わっているのでworld2camera()ではなくcamera2image()を呼び出す\n",
    "        image_coords = PinholeCamera.camera2image(world_coords[..., :3], intrinsic, batch=True)\n",
    "        # PyTorchのgrid samplingはcv2.remapとは異なり、座標が[-1, 1]に正規化されたものを入力する\n",
    "        image_coords[..., 0] = image_coords[..., 0] / image_coords.shape[2] * 2 - 1\n",
    "        image_coords[..., 1] = image_coords[..., 1] / image_coords.shape[1] * 2 - 1\n",
    "        grid = image_coords\n",
    "        image_warped = F.grid_sample(image_source, grid, align_corners=False)\n",
    "        return image_warped\n",
    "                \n",
    "    @lru_cache(None)\n",
    "    def create_image_coords(self, map_shape):\n",
    "        \"\"\" 各画素に対する画像座標を生成する \"\"\"\n",
    "        xi = torch.arange(0, map_shape[2], 1)\n",
    "        yi = torch.arange(0, map_shape[1], 1)\n",
    "        coord_x, coord_y = torch.meshgrid(xi, yi, indexing=\"xy\")\n",
    "        image_coords = torch.stack([coord_x, coord_y], axis=-1)\n",
    "        image_coords = image_coords.float()        \n",
    "        image_coords = image_coords.unsqueeze(0).repeat(map_shape[0], 1, 1, 1) # バッチ化\n",
    "        return image_coords\n",
    "\n",
    "\n",
    "class SmoothnessLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        depth = y_pred[f\"inv_depth_0\"]\n",
    "        gradients_y = torch.mean(torch.abs(inv_depth[..., :-1,  :] - inv_depth[..., 1:,  :]))\n",
    "        gradients_x = torch.mean(torch.abs(inv_depth[..., :  ,:-1] - inv_depth[...,  :, 1:]))\n",
    "        return (gradients_x + gradients_y) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3972718-5505-49f7-8981-826e8090b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "photometric_loss = PhotometricLoss(frame_inds=frame_inds)\n",
    "smoothness_loss = SmoothnessLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14ea33-2d55-4013-a18f-97538894aef9",
   "metadata": {},
   "source": [
    "# Pose Utils\n",
    "pose netが推定した姿勢の変化量(6DoF)を4x4の行列に変換する。  \n",
    "`torchgeometry`の`rvec_to_pose`を使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da6b39d-b038-4d49-9a03-b1dcb9a89ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 実行例　6DoFが4x4の行列に変換されることを確認する\n",
    "tgm.rtvec_to_pose(torch.rand(3, 6)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17cfbf-b629-42e9-bb85-c65115e52e64",
   "metadata": {},
   "source": [
    "## training loop\n",
    "ここからようやく学習を実行する。<br>\n",
    "この学習はGTX1060（6GB RAM）で動作することが確認できている。<br>\n",
    "より大きなRAMを搭載したGPUをつかえばバッチ数、フレーム数（frame_inds）を増やし、学習を安定化させることができるはずである。\n",
    "\n",
    "現状は学習の初期が非常に不安定であり、depth meanがnanになってしまうことが多い。<br>\n",
    "学習が安定するまで何度か実行し直す必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db84c088-28a0-4d56-90cb-15136705f6dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sequences\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2229/3697472992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# TODO: datasetモジュール作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPandaset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPandaset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/working/monocular_depth_estimation_from_scratch_pandaset/src/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, variations, cameras, frame_inds)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_inds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading sequences\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/working/monocular_depth_estimation_from_scratch_pandaset/src/dataset.py\u001b[0m in \u001b[0;36mload_sequences\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m                         \u001b[0mextrinsics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpose_to_extrinsic_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpose_json_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0mintrinsics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintrinsic_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintrinsic_json_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextrinsics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintrinsics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": [
    "# 学習用のデータセットクラスの読み込みを行う\n",
    "# TODO: datasetモジュール作成 \n",
    "from dataset import Pandaset\n",
    "train_dataset = Pandaset(root_dir=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31f16e43-da90-4d76-97a7-64c7662ce443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:  [(PosixPath('../data/001/camera/back_camera/00.jpg'), array([-0.7219474 ,  0.69019829,  0.04917599,  0.03161158]), array([933.4667,   0.    , 896.4692])), (PosixPath('../data/001/camera/back_camera/01.jpg'), array([-0.03736028,  0.03208378, -0.99878668,  1.67161498]), array([  0.    , 934.6754, 507.3557]))]\n",
      "sample len: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rgb_-1': array([[[101, 114,  58],\n",
       "         [ 93, 106,  50],\n",
       "         [ 78,  89,  33],\n",
       "         ...,\n",
       "         [ 76,  93, 103],\n",
       "         [ 68,  85,  95],\n",
       "         [ 58,  75,  85]],\n",
       " \n",
       "        [[ 94, 111,  56],\n",
       "         [ 92, 109,  54],\n",
       "         [ 86, 101,  46],\n",
       "         ...,\n",
       "         [ 74,  91, 101],\n",
       "         [ 66,  83,  93],\n",
       "         [ 56,  73,  83]],\n",
       " \n",
       "        [[ 88, 113,  56],\n",
       "         [ 88, 113,  56],\n",
       "         [ 86, 109,  53],\n",
       "         ...,\n",
       "         [ 72,  89,  99],\n",
       "         [ 63,  80,  90],\n",
       "         [ 53,  70,  80]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 93, 114, 135],\n",
       "         [ 93, 114, 135],\n",
       "         [ 93, 114, 135],\n",
       "         ...,\n",
       "         [ 93, 115, 136],\n",
       "         [ 93, 115, 136],\n",
       "         [ 93, 115, 136]],\n",
       " \n",
       "        [[ 93, 114, 135],\n",
       "         [ 93, 114, 135],\n",
       "         [ 93, 114, 135],\n",
       "         ...,\n",
       "         [ 93, 115, 136],\n",
       "         [ 93, 115, 136],\n",
       "         [ 93, 115, 136]],\n",
       " \n",
       "        [[ 93, 114, 135],\n",
       "         [ 93, 114, 135],\n",
       "         [ 93, 114, 135],\n",
       "         ...,\n",
       "         [ 93, 115, 136],\n",
       "         [ 93, 115, 136],\n",
       "         [ 93, 115, 136]]], dtype=uint8),\n",
       " 'extrinsic_-1': array([-0.7219474 ,  0.69019829,  0.04917599,  0.03161158]),\n",
       " 'intrinsic_-1': array([933.4667,   0.    , 896.4692]),\n",
       " 'rgb_0': array([[[ 29,  44,  41],\n",
       "         [ 28,  43,  38],\n",
       "         [ 25,  41,  31],\n",
       "         ...,\n",
       "         [ 43,  58,  89],\n",
       "         [ 49,  64,  95],\n",
       "         [ 53,  68,  99]],\n",
       " \n",
       "        [[ 26,  41,  36],\n",
       "         [ 24,  39,  32],\n",
       "         [ 22,  38,  27],\n",
       "         ...,\n",
       "         [ 57,  72, 101],\n",
       "         [ 59,  74, 103],\n",
       "         [ 59,  74, 103]],\n",
       " \n",
       "        [[ 25,  42,  34],\n",
       "         [ 24,  41,  31],\n",
       "         [ 21,  39,  25],\n",
       "         ...,\n",
       "         [ 72,  89, 115],\n",
       "         [ 67,  84, 110],\n",
       "         [ 61,  78, 104]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 92, 114, 135],\n",
       "         [ 94, 116, 137],\n",
       "         [ 96, 118, 139],\n",
       "         ...,\n",
       "         [ 92, 121, 135],\n",
       "         [ 92, 121, 135],\n",
       "         [ 92, 121, 135]],\n",
       " \n",
       "        [[ 93, 115, 136],\n",
       "         [ 96, 118, 139],\n",
       "         [ 98, 120, 141],\n",
       "         ...,\n",
       "         [ 92, 121, 135],\n",
       "         [ 92, 121, 135],\n",
       "         [ 92, 121, 135]],\n",
       " \n",
       "        [[ 95, 117, 138],\n",
       "         [ 97, 119, 140],\n",
       "         [100, 122, 143],\n",
       "         ...,\n",
       "         [ 92, 121, 135],\n",
       "         [ 92, 121, 135],\n",
       "         [ 92, 121, 135]]], dtype=uint8),\n",
       " 'extrinsic_0': array([-0.03736028,  0.03208378, -0.99878668,  1.67161498]),\n",
       " 'intrinsic_0': array([  0.    , 934.6754, 507.3557])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adf53361-5961-4031-bd18-b87f538ec37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Transform(Dataset):\n",
    "    \"\"\"\n",
    "    ネットワークへの入力に適切な形に変換するクラス。\n",
    "    主に画像のリサイズとそれに伴う内部パラメタの補正を行う。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):        \n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.crop = (45, 43, 1197, 331) # (x0, y0, x1, y1)\n",
    "        self.scale = 1.0 / 3.0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        print(data[\"extrinsic_-1\"].shape)\n",
    "        for key in data.keys():\n",
    "            if key.startswith(\"rgb_\") or key.startswith(\"depth_\"):\n",
    "                # 画像と深度を変換する\n",
    "                image = data[key]\n",
    "                # クロップと縮小。CNNに入力する都合でキリの良い画素数に変更する必要があり、\n",
    "                # ここでは1242x375--crop-->1152x288--resize-->384x96としている。\n",
    "                image = image[self.crop[1]:self.crop[3], self.crop[0]:self.crop[2]]\n",
    "                orig_shape = image.shape\n",
    "                dest_size = (int(orig_shape[1] * self.scale), int(orig_shape[0] * self.scale))\n",
    "                image = cv2.resize(image, dest_size, interpolation=cv2.INTER_LINEAR)\n",
    "                data[key] = torch.tensor(image).float()\n",
    "                if key.startswith(\"rgb_\"):\n",
    "                    data[key] = data[key].permute(2, 0, 1) # (B, H, W, C) -> (B, C, H, W)\n",
    "                    data[key] /= 255.0 # normalize\n",
    "            elif key.startswith(\"intrinsic_\"):\n",
    "                # 画像がリサイズとクロップに合わせて内部パラメタを補正する\n",
    "                intrinsic = data[key]\n",
    "                intrinsic[0, 2] = intrinsic[0, 2] - self.crop[0]\n",
    "                intrinsic[1, 2] = intrinsic[1, 2] - self.crop[1]\n",
    "                intrinsic[:2, :] *= self.scale\n",
    "                data[key] = torch.tensor(intrinsic).float()\n",
    "            else:\n",
    "                data[key] = torch.tensor(data[key]).float()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(Transform(train_dataset), batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e08db7e-1d49-45c0-8f9d-43faefb2010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1745/1482942772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miter_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1745/952784042.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# 画像がリサイズとクロップに合わせて内部パラメタを補正する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mintrinsic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "iter_loader = iter(train_dataloader)\n",
    "next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "482ab7e1-0427-418e-a1eb-9fc225c2c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "*** training start ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1745/3299647945.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;31m# GPUにバッチを転送する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prepare batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1745/466480360.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# 画像がリサイズとクロップに合わせて内部パラメタを補正する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mintrinsic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mintrinsic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR   \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "# GPUを使う場合（マルチGPUは非対応）\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# num_batch_accumulation = 4\n",
    "\n",
    "depth_net.train().to(device)\n",
    "pose_net.train().to(device)\n",
    "\n",
    "# optimizerを定義。depth_netとpose_netの２つのネットワークのパラメタを渡す。\n",
    "optimizer = Adam([\n",
    "    {\"params\": depth_net.parameters()},\n",
    "    {\"params\": pose_net.parameters()}],\n",
    "    lr=lr,\n",
    "    )\n",
    "\n",
    "# learning rate schecdulerを定義。徐々にlrを減衰させる。\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.5)\n",
    "\n",
    "print(\"*** training start ***\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    with tqdm(train_dataloader) as pbar:\n",
    "        for j, batch in enumerate(pbar):\n",
    "            # GPUにバッチを転送する\n",
    "            batch = {k:v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # 深度を推定する\n",
    "            inv_depth = depth_net(batch[\"rgb_0\"])\n",
    "            inv_depth = F.relu(inv_depth)\n",
    "            inv_depth = inv_depth.squeeze(1)\n",
    "            inv_depth = (inv_depth + 1e-2) / (1 + 1e-2) # inverse depthの最小値（最長深度）を1e-2（100）とする。\n",
    "            depth = 1 / inv_depth\n",
    "\n",
    "            # 姿勢を推定する\n",
    "            image_concat = torch.cat([batch[\"rgb_0\"], batch[\"rgb_-1\"]], axis=1) # ソースとターゲットの２枚の画像を同時に入力する\n",
    "            pose = torch.tanh(pose_net(image_concat))\n",
    "            rotation, translation = pose[..., 0:3], pose[..., 3:]\n",
    "            rotation = rotation * math.pi # 各軸の回転を最大でPiに限定する\n",
    "            translation = translation * 5.0 # 各軸の並進を最大で5.0に限定する\n",
    "            rtmat = tgm.rtvec_to_pose(torch.cat([rotation, translation], dim=-1))\n",
    "\n",
    "            # Lossを計算する\n",
    "            y = {k:v for k, v in batch.items() if k.startswith(\"rgb_\") or k.startswith(\"intrinsic_\")}\n",
    "            y_pred = {\n",
    "                \"depth_0\": depth,\n",
    "                \"inv_depth_0\": inv_depth,\n",
    "                \"extrinsic_-1\": rtmat,\n",
    "            }\n",
    "            loss_ph, image_warped, automask = photometric_loss(y, y_pred)\n",
    "            loss_sm = smoothness_loss(y, y_pred)\n",
    "            loss = (loss_ph * 0.95) + (loss_sm * 0.05)\n",
    "            loss.backward()\n",
    "\n",
    "            # if (j + 1) % num_batch_accumulation == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if j % 200 == 0:\n",
    "                # デバッグのために１枚分の出力を表示する\n",
    "                plt.figure(figsize=(20, 20))\n",
    "                ax_image = plt.subplot(4, 1, 1)\n",
    "                ax_warped = plt.subplot(4, 1, 2)\n",
    "                ax_depth = plt.subplot(4, 1, 3)\n",
    "                ax_automask = plt.subplot(4, 1, 4)\n",
    "\n",
    "                ax_image.set_title(\"target image\")\n",
    "                ax_image.imshow(batch[\"rgb_0\"][0].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "                \n",
    "                ax_warped.set_title(\"warped image (source to target)\")\n",
    "                ax_warped.imshow(image_warped[-1][0].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "\n",
    "                ax_depth.set_title(\"inverse depth map\")\n",
    "                ax_depth.imshow(inv_depth[0].detach().cpu().numpy())\n",
    "\n",
    "                ax_automask.set_title(\"auto-masking (if disabled, it's filled by zeros)\")\n",
    "                ax_automask.imshow(automask[0].detach().cpu().numpy())\n",
    "                \n",
    "                os.makedirs(\"debug\", exist_ok=True)\n",
    "                plt.savefig(f\"debug/epoch_{i}_iter_{j}_output.jpeg\")\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "            # プログレスバーに現在の状態を出力する\n",
    "            pbar.set_description(\n",
    "                f\"[Epoch {i}] loss (ph): {loss_ph:0.3f}, \" \\\n",
    "                f\"loss (sm) {loss_sm:0.3f}, \" \\\n",
    "                f\"depth mean {depth.mean():0.3f}, \" \\\n",
    "                f\"lr {scheduler.get_last_lr()[0]:0.6f}, \" \\\n",
    "                f\"trans mag {torch.linalg.vector_norm(pose[..., 3:], ord=2, dim=-1).mean():0.3f}\")\n",
    "\n",
    "    os.makedirs(\"../ckpt/\", exist_ok=True)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": {\n",
    "                \"depth_net\": depth_net.state_dict(),\n",
    "                \"pose_net\": pose_net.state_dict(),\n",
    "            }\n",
    "        }, f\"../ckpt/models_{i}_epoch.pt\"    \n",
    "    )\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186a8c1-f7ab-45cc-8fc8-8086d4e58e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
