{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cee9fa6-0b22-47db-b970-22ae3b217b86",
   "metadata": {},
   "source": [
    "# 単眼深度推定モデルを学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc728c2-a249-4b7f-99f3-4966fce0c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1beef-ffe6-443b-8bef-8a1ad5935cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed204ba7-d45f-45f6-aa64-e13bdbf9d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_inds = [0, -1] # 隣接フレームの番号\n",
    "epochs = 1 #100\n",
    "lr = 0.0004\n",
    "batch_size = 2 #32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33003d41-9f92-4460-8629-f0a721f67be6",
   "metadata": {},
   "source": [
    "# ネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdd6ee-5b6f-484e-b1a7-330485bafbfc",
   "metadata": {},
   "source": [
    "学習するネットワークは現在のフレームの深度マップを推定するものと、フレーム間の姿勢の変化を推定するものの２つである。  \n",
    "ここではそれぞれdepth netとpose netと呼ぶ。\n",
    "- depth netは深度マップを推定するネットワーク\n",
    "- pose netは回転（X軸、Y軸、Z軸）と並進（X, Y, Z）の合計6個の数値を推定するネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9e2f1-665b-4277-9cbd-b2aad917b78d",
   "metadata": {},
   "source": [
    "## depth net\n",
    "ここではネットワークの詳細に関心はないたえ、`segmentation_models_pytorch`を使いU-Netを定義する。  \n",
    "最新の深度推定モデルは複数の解像度の深度マップを推定するのが一般的だが、ここではGPUのメモリを節約するために、１枚のみとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8bbf17-f196-4464-b4dd-59113267f8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2185, -0.0506,  0.9428,  ..., -1.2236,  2.5891,  4.5206],\n",
       "          [ 0.0694,  1.5021,  0.6834,  ...,  4.8655,  3.4201, -1.4867],\n",
       "          [-0.7716,  0.7986,  0.1954,  ..., -4.0873, -0.1974, -7.3232],\n",
       "          ...,\n",
       "          [ 3.8710,  1.7844,  1.2969,  ...,  2.9854,  3.9457,  7.0399],\n",
       "          [ 4.7185,  2.1211,  0.4900,  ..., -1.5433, -3.3264,  1.5072],\n",
       "          [ 2.9979,  3.0880,  1.9911,  ...,  2.5324,  1.1391,  0.8447]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "depth_net = smp.Unet(\"efficientnet-b0\", in_channels=3, classes=1, activation=None)\n",
    "\n",
    "# 入出力確認\n",
    "depth_net(torch.zeros(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602f629-eb17-4f27-bc0c-a73b8ba4c002",
   "metadata": {},
   "source": [
    "## pose net\n",
    "pose netに関してもネットワークの詳細に関心がないため、`torchvision`の学習済みモデルを持ってきて、部分的にレイヤーを差し替える。\n",
    "現在フレームと隣接フレームの２枚を入力し、その２枚の間で発生した姿勢の変化量を推定する。  \n",
    "- 入力: 2枚の画像はチャンネルの次元で結合して合計で6チャンネルのテンソルとして入力を行う。\n",
    "- 出力： 出力は並進(X軸、Y軸、Z軸)と回転(X軸、Y軸、Z軸)の6DoFである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b456a6-fb3c-4a42-9f4a-268694766544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3565e-04, -3.5509e-04,  2.3634e-04, -2.3376e-05, -2.2161e-04,\n",
       "         -6.7018e-04]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch.nn import Linear, Conv2d\n",
    "\n",
    "pose_net = resnet18(pretrained=True)\n",
    "pose_net.conv1 = Conv2d(in_channels=3*2, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "pose_net.fc = Linear(in_features=512, out_features=6)\n",
    "\n",
    "# 初期はすべて0を出力したほうが都合がよいので、weightとbiasを0クリアしておく\n",
    "# reproj_loss_ += torch.randn(reproj_loss_.shape).to(device=reproj_loss_.device) * 1e-3\n",
    "pose_net.fc.weight.data = torch.randn(pose_net.fc.weight.shape) * 1e-5\n",
    "pose_net.fc.bias.data = torch.randn(pose_net.fc.bias.shape) * 1e-5\n",
    "\n",
    "# 入出力確認\n",
    "pose_net(torch.zeros(1, 3*2, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91511f1-ef0d-4e4c-9780-0cf7e31d8fe8",
   "metadata": {},
   "source": [
    "# Loss関数\n",
    "ここでは、Phtometric lossとSmoothness lossを定義する。  \n",
    "Photometric lossは推定した深度と姿勢変化からSourceの画像をTargetの画像に合わせて一致しているかどうかをl1とSSIMで測る。  \n",
    "また、[Monodepth2](https://arxiv.org/abs/1806.01260)で提案されたOcculusionや動体から生じる原理的に復元不可能な画素に対するLossの計算を回避するauto-maskingを導入した。  \n",
    "Smoothness lossはPhotometric lossの復元誤差が濃淡が平滑な領域で勾配を得にくいという問題を解決するために導入した。  \n",
    "このlossは近接ピクセルがおおよそ同じような深度を持っている（物体の境界以外は）という仮定のもと、それをLossとして与えるものである。  \n",
    "実装は様々あるが、ここではX,Y方向の差分のみから計算する比較的にシンプルなものを用いる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ea5fd3-5bd7-4897-9ce2-6e9c833af4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d01700d-7ee7-4d90-a914-bb25801e597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from camera import PinholeCamera\n",
    "from functools import lru_cache\n",
    "import torchgeometry as tgm\n",
    "\n",
    "\n",
    "class PhotometricLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, frame_inds, weights=[0.6, 0.4], automasking=False):\n",
    "        \"\"\"\n",
    "        Photometric loss\n",
    "        weights: l1とssimのlossに対する重みを指定する。デフォルトは[0.4, 0.6]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.frame_inds = frame_inds\n",
    "        self.l1_loss = torch.nn.L1Loss(reduction=\"none\")\n",
    "        self.ssim_loss = tgm.losses.SSIM(reduction='none', window_size=3)\n",
    "        self.weights = weights\n",
    "        self.automasking = automasking\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        image_target = y[\"rgb_0\"]\n",
    "        depth = y_pred[f\"depth_0\"]\n",
    "        intrinsic = y[\"intrinsic_0\"]\n",
    "\n",
    "        reproj_loss = []\n",
    "        image_warped = {}\n",
    "        for idx in self.frame_inds:\n",
    "            if idx == 0: # ターゲット同士は比較しない\n",
    "                continue\n",
    "            image_source = y[f\"rgb_{idx}\"]\n",
    "            extrinsic_src2tgt = y_pred[f\"extrinsic_{idx}\"]\n",
    "            image_warped_ = self.warp(image_source, depth, intrinsic, extrinsic_src2tgt)\n",
    "            l1_loss = self.l1_loss(image_warped_, image_target)\n",
    "            ssim_loss = self.ssim_loss(image_warped_, image_target)\n",
    "            reproj_loss_ = l1_loss * self.weights[0] + ssim_loss * self.weights[1]\n",
    "            reproj_loss_ = torch.mean(reproj_loss_, dim=1) # auto-maskingで扱いやすいようにチャンネルの次元を潰しておく\n",
    "            reproj_loss.append(reproj_loss_)\n",
    "            image_warped[idx] = image_warped_\n",
    "        \n",
    "        if self.automasking:\n",
    "            # auto-masking (https://arxiv.org/pdf/1806.01260.pdf) 何も変更を加えないSource画像を利用する。\n",
    "            for idx in self.frame_inds:\n",
    "                if idx == 0: # ターゲット同士は比較しない\n",
    "                    continue\n",
    "                image_source = y[f\"rgb_{idx}\"]\n",
    "                l1_loss = self.l1_loss(image_source, image_target)\n",
    "                ssim_loss = self.ssim_loss(image_source, image_target)\n",
    "                reproj_loss_ = l1_loss * self.weights[0] + ssim_loss * self.weights[1]\n",
    "                # 平坦な領域ではWarpされたものと何も変更を加えないものでLossが全くおなじになってしまう画素が生じる可能性があるので微小な乱数を加える\n",
    "                reproj_loss_ += torch.randn(reproj_loss_.shape).to(device=reproj_loss_.device) * 1e-3\n",
    "                reproj_loss_ = torch.mean(reproj_loss_, dim=1) # auto-maskingで扱いやすいようにチャンネルの次元を潰しておく\n",
    "                reproj_loss.append(reproj_loss_)\n",
    "\n",
    "            reproj_loss = torch.stack(reproj_loss, dim=1)\n",
    "            loss, min_inds = torch.min(reproj_loss, dim=1)\n",
    "            automask = (min_inds >= (reproj_loss.shape[1] // 2)).float()\n",
    "            loss = reproj_loss.mean()\n",
    "        else:\n",
    "            loss = torch.stack(reproj_loss, dim=1)\n",
    "            automask = torch.zeros_like(loss).squeeze(1) # dummy\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        return loss, image_warped, automask\n",
    "\n",
    "    def warp(self, image_source, depth, intrinsic, extrinsic_src2tgt):        \n",
    "        \"\"\" 推定された深度と姿勢からソースをターゲットに重ね合わせる \"\"\"\n",
    "        image_coords = self.create_image_coords(depth.shape)\n",
    "        image_coords = image_coords.to(depth.device)\n",
    "        world_coords = PinholeCamera.image2world(image_coords, intrinsic, extrinsic_src2tgt, depth, batch=True)        \n",
    "        # これまでノートとは異なりターゲットのカメラへの座標変換が終わっているのでworld2camera()ではなくcamera2image()を呼び出す\n",
    "        image_coords = PinholeCamera.camera2image(world_coords[..., :3], intrinsic, batch=True)\n",
    "        # PyTorchのgrid samplingはcv2.remapとは異なり、座標が[-1, 1]に正規化されたものを入力する\n",
    "        image_coords[..., 0] = image_coords[..., 0] / image_coords.shape[2] * 2 - 1\n",
    "        image_coords[..., 1] = image_coords[..., 1] / image_coords.shape[1] * 2 - 1\n",
    "        grid = image_coords\n",
    "        image_warped = F.grid_sample(image_source, grid, align_corners=False)\n",
    "        return image_warped\n",
    "                \n",
    "    @lru_cache(None)\n",
    "    def create_image_coords(self, map_shape):\n",
    "        \"\"\" 各画素に対する画像座標を生成する \"\"\"\n",
    "        xi = torch.arange(0, map_shape[2], 1)\n",
    "        yi = torch.arange(0, map_shape[1], 1)\n",
    "        coord_x, coord_y = torch.meshgrid(xi, yi, indexing=\"xy\")\n",
    "        image_coords = torch.stack([coord_x, coord_y], axis=-1)\n",
    "        image_coords = image_coords.float()        \n",
    "        image_coords = image_coords.unsqueeze(0).repeat(map_shape[0], 1, 1, 1) # バッチ化\n",
    "        return image_coords\n",
    "\n",
    "\n",
    "class SmoothnessLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        depth = y_pred[f\"inv_depth_0\"]\n",
    "        gradients_y = torch.mean(torch.abs(inv_depth[..., :-1,  :] - inv_depth[..., 1:,  :]))\n",
    "        gradients_x = torch.mean(torch.abs(inv_depth[..., :  ,:-1] - inv_depth[...,  :, 1:]))\n",
    "        return (gradients_x + gradients_y) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3972718-5505-49f7-8981-826e8090b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "photometric_loss = PhotometricLoss(frame_inds=frame_inds)\n",
    "smoothness_loss = SmoothnessLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14ea33-2d55-4013-a18f-97538894aef9",
   "metadata": {},
   "source": [
    "# Pose Utils\n",
    "pose netが推定した姿勢の変化量(6DoF)を4x4の行列に変換する。  \n",
    "`torchgeometry`の`rvec_to_pose`を使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da6b39d-b038-4d49-9a03-b1dcb9a89ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 実行例　6DoFが4x4の行列に変換されることを確認する\n",
    "tgm.rtvec_to_pose(torch.rand(3, 6)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17cfbf-b629-42e9-bb85-c65115e52e64",
   "metadata": {},
   "source": [
    "## training loop\n",
    "ここからようやく学習を実行する。<br>\n",
    "この学習はGTX1060（6GB RAM）で動作することが確認できている。<br>\n",
    "より大きなRAMを搭載したGPUをつかえばバッチ数、フレーム数（frame_inds）を増やし、学習を安定化させることができるはずである。\n",
    "\n",
    "現状は学習の初期が非常に不安定であり、depth meanがnanになってしまうことが多い。<br>\n",
    "学習が安定するまで何度か実行し直す必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db84c088-28a0-4d56-90cb-15136705f6dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sequences\n"
     ]
    }
   ],
   "source": [
    "# 学習用のデータセットクラスの読み込みを行う\n",
    "from dataset import Pandaset\n",
    "train_dataset = Pandaset(root_dir=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0c953fe-8bed-4173-8b56-4ae89c24cffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"rgb_0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a0575f7-47e8-44e6-81e7-b1212f5e4a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[933.4667,   0.    , 896.4692],\n",
       "       [  0.    , 934.6754, 507.3557],\n",
       "       [  0.    ,   0.    ,   1.    ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"intrinsic_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf53361-5961-4031-bd18-b87f538ec37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Transform(Dataset):\n",
    "    \"\"\"\n",
    "    ネットワークへの入力に適切な形に変換するクラス。\n",
    "    主に画像のリサイズとそれに伴う内部パラメタの補正を行う。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):        \n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.crop = (45, 43, 700, 331) # (x0, y0, x1, y1)\n",
    "        self.scale = 1.0 / 3.0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        for key in data.keys():\n",
    "            if key.startswith(\"rgb_\") or key.startswith(\"depth_\"):\n",
    "                # 画像と深度を変換する\n",
    "                image = data[key]\n",
    "                # クロップと縮小。CNNに入力する都合でキリの良い画素数に変更する必要があり、\n",
    "                # ここでは1242x375--crop-->1152x288--resize-->384x96としている。\n",
    "                image = image[self.crop[1]:self.crop[3], self.crop[0]:self.crop[2]]\n",
    "                orig_shape = image.shape\n",
    "                dest_size = (int(orig_shape[1] * self.scale), int(orig_shape[0] * self.scale))\n",
    "                image = cv2.resize(image, dest_size, interpolation=cv2.INTER_LINEAR)\n",
    "                data[key] = torch.tensor(image).float()\n",
    "                if key.startswith(\"rgb_\"):\n",
    "                    data[key] = data[key].permute(2, 0, 1) # (B, H, W, C) -> (B, C, H, W)\n",
    "                    data[key] /= 255.0 # normalize\n",
    "            elif key.startswith(\"intrinsic_\"):\n",
    "                # 画像がリサイズとクロップに合わせて内部パラメタを補正する\n",
    "                intrinsic = data[key]\n",
    "                intrinsic[0, 2] = intrinsic[0, 2] - self.crop[0]\n",
    "                intrinsic[1, 2] = intrinsic[1, 2] - self.crop[1]\n",
    "                intrinsic[:2, :] *= self.scale\n",
    "                data[key] = torch.tensor(intrinsic).float()\n",
    "            else:\n",
    "                data[key] = torch.tensor(data[key]).float()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(Transform(train_dataset), batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e08db7e-1d49-45c0-8f9d-43faefb2010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rgb_-1': tensor([[[[0.1451, 0.1451, 0.1725,  ..., 0.6235, 0.6235, 0.6235],\n",
       "           [0.2392, 0.2431, 0.2314,  ..., 0.6235, 0.6235, 0.6235],\n",
       "           [0.2667, 0.2667, 0.2784,  ..., 0.6235, 0.6235, 0.6196],\n",
       "           ...,\n",
       "           [0.3569, 0.3490, 0.3451,  ..., 0.2667, 0.3686, 0.4510],\n",
       "           [0.3569, 0.3412, 0.3490,  ..., 0.2549, 0.2902, 0.3098],\n",
       "           [0.3725, 0.3529, 0.3451,  ..., 0.2667, 0.2667, 0.3294]],\n",
       " \n",
       "          [[0.1608, 0.1608, 0.1882,  ..., 0.6902, 0.6902, 0.6902],\n",
       "           [0.2588, 0.2471, 0.2314,  ..., 0.6902, 0.6902, 0.6902],\n",
       "           [0.2784, 0.2745, 0.2863,  ..., 0.6902, 0.6902, 0.6863],\n",
       "           ...,\n",
       "           [0.3686, 0.3647, 0.3608,  ..., 0.3137, 0.4118, 0.5137],\n",
       "           [0.3686, 0.3569, 0.3647,  ..., 0.3020, 0.3333, 0.3725],\n",
       "           [0.3843, 0.3647, 0.3569,  ..., 0.3137, 0.3098, 0.3922]],\n",
       " \n",
       "          [[0.1569, 0.1569, 0.1843,  ..., 0.7922, 0.7922, 0.7922],\n",
       "           [0.2353, 0.2275, 0.2235,  ..., 0.7922, 0.7922, 0.7922],\n",
       "           [0.2431, 0.2235, 0.2431,  ..., 0.7922, 0.7922, 0.7882],\n",
       "           ...,\n",
       "           [0.3255, 0.3059, 0.3020,  ..., 0.2745, 0.3882, 0.4745],\n",
       "           [0.3333, 0.3020, 0.3098,  ..., 0.2627, 0.3098, 0.3333],\n",
       "           [0.3490, 0.3216, 0.3137,  ..., 0.2745, 0.2863, 0.3529]]],\n",
       " \n",
       " \n",
       "         [[[0.1686, 0.1294, 0.3490,  ..., 0.4471, 0.4471, 0.4431],\n",
       "           [0.1255, 0.1843, 0.3647,  ..., 0.4510, 0.4510, 0.4471],\n",
       "           [0.2667, 0.2902, 0.3412,  ..., 0.4471, 0.4471, 0.4471],\n",
       "           ...,\n",
       "           [0.3255, 0.4157, 0.4431,  ..., 0.3647, 0.3569, 0.3569],\n",
       "           [0.3373, 0.4314, 0.4196,  ..., 0.3647, 0.3686, 0.3647],\n",
       "           [0.3059, 0.2902, 0.2706,  ..., 0.3882, 0.3529, 0.3255]],\n",
       " \n",
       "          [[0.1843, 0.1255, 0.3569,  ..., 0.5255, 0.5255, 0.5216],\n",
       "           [0.1529, 0.1843, 0.3843,  ..., 0.5294, 0.5294, 0.5255],\n",
       "           [0.3020, 0.2902, 0.3569,  ..., 0.5255, 0.5255, 0.5255],\n",
       "           ...,\n",
       "           [0.3647, 0.4627, 0.4863,  ..., 0.3686, 0.3608, 0.3608],\n",
       "           [0.3765, 0.4627, 0.4510,  ..., 0.3686, 0.3725, 0.3686],\n",
       "           [0.3451, 0.3098, 0.2902,  ..., 0.3922, 0.3569, 0.3294]],\n",
       " \n",
       "          [[0.1804, 0.1176, 0.3373,  ..., 0.6314, 0.6314, 0.6275],\n",
       "           [0.1255, 0.1765, 0.3686,  ..., 0.6353, 0.6353, 0.6314],\n",
       "           [0.2902, 0.2902, 0.3529,  ..., 0.6235, 0.6235, 0.6235],\n",
       "           ...,\n",
       "           [0.3333, 0.4078, 0.4314,  ..., 0.3373, 0.3294, 0.3294],\n",
       "           [0.3451, 0.4196, 0.4078,  ..., 0.3373, 0.3412, 0.3373],\n",
       "           [0.3137, 0.2824, 0.2627,  ..., 0.3608, 0.3255, 0.2980]]]]),\n",
       " 'extrinsic_-1': tensor([[[[-3.8377e-01,  9.2340e-01,  6.6303e-03, -9.5377e+00],\n",
       "           [-2.9391e-02, -5.0381e-03, -9.9956e-01,  1.5191e+00],\n",
       "           [-9.2296e-01, -3.8379e-01,  2.9074e-02,  4.1406e-02],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-6.1924e-01,  7.8520e-01,  8.1134e-04, -3.2676e+01],\n",
       "           [-3.5173e-02, -2.6707e-02, -9.9902e-01,  1.5043e+00],\n",
       "           [-7.8441e-01, -6.1867e-01,  4.4156e-02,  5.8081e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]]]),\n",
       " 'intrinsic_-1': tensor([[[310.1505,   0.0000, 315.5628],\n",
       "          [  0.0000, 310.0297, 166.2019],\n",
       "          [  0.0000,   0.0000,   1.0000]],\n",
       " \n",
       "         [[310.1505,   0.0000, 315.5628],\n",
       "          [  0.0000, 310.0297, 166.2019],\n",
       "          [  0.0000,   0.0000,   1.0000]]]),\n",
       " 'rgb_0': tensor([[[[0.1882, 0.1608, 0.1373,  ..., 0.6235, 0.6235, 0.6275],\n",
       "           [0.1569, 0.1608, 0.1843,  ..., 0.6235, 0.6235, 0.6275],\n",
       "           [0.2667, 0.2549, 0.2510,  ..., 0.6275, 0.6275, 0.6275],\n",
       "           ...,\n",
       "           [0.3451, 0.3490, 0.3490,  ..., 0.3216, 0.1961, 0.3569],\n",
       "           [0.3451, 0.3490, 0.3451,  ..., 0.2314, 0.2902, 0.4549],\n",
       "           [0.3608, 0.3647, 0.3373,  ..., 0.2118, 0.2078, 0.3059]],\n",
       " \n",
       "          [[0.2118, 0.1843, 0.1569,  ..., 0.6863, 0.6863, 0.6902],\n",
       "           [0.1608, 0.1765, 0.1882,  ..., 0.6863, 0.6863, 0.6902],\n",
       "           [0.2706, 0.2627, 0.2588,  ..., 0.6902, 0.6902, 0.6902],\n",
       "           ...,\n",
       "           [0.3529, 0.3569, 0.3647,  ..., 0.4000, 0.2784, 0.4275],\n",
       "           [0.3569, 0.3608, 0.3686,  ..., 0.3294, 0.3961, 0.5451],\n",
       "           [0.3725, 0.3882, 0.3686,  ..., 0.2667, 0.2588, 0.3529]],\n",
       " \n",
       "          [[0.2118, 0.1843, 0.1686,  ..., 0.7882, 0.7882, 0.7922],\n",
       "           [0.1686, 0.1725, 0.1961,  ..., 0.7882, 0.7882, 0.7922],\n",
       "           [0.2510, 0.2431, 0.2392,  ..., 0.7922, 0.7922, 0.7922],\n",
       "           ...,\n",
       "           [0.3020, 0.3059, 0.3098,  ..., 0.3529, 0.2588, 0.4275],\n",
       "           [0.3137, 0.3176, 0.3216,  ..., 0.2431, 0.3373, 0.5137],\n",
       "           [0.3373, 0.3490, 0.3255,  ..., 0.1765, 0.1882, 0.3059]]],\n",
       " \n",
       " \n",
       "         [[[0.2000, 0.3255, 0.2941,  ..., 0.4549, 0.4549, 0.4549],\n",
       "           [0.2980, 0.2902, 0.2863,  ..., 0.4549, 0.4549, 0.4549],\n",
       "           [0.3216, 0.3059, 0.2824,  ..., 0.4549, 0.4549, 0.4549],\n",
       "           ...,\n",
       "           [0.3098, 0.3333, 0.4588,  ..., 0.3804, 0.3843, 0.3451],\n",
       "           [0.2588, 0.3020, 0.3412,  ..., 0.3686, 0.3451, 0.3686],\n",
       "           [0.3059, 0.3294, 0.3294,  ..., 0.4549, 0.3961, 0.3333]],\n",
       " \n",
       "          [[0.2235, 0.3686, 0.3451,  ..., 0.5216, 0.5216, 0.5216],\n",
       "           [0.3373, 0.3333, 0.3373,  ..., 0.5216, 0.5216, 0.5216],\n",
       "           [0.3608, 0.3255, 0.3216,  ..., 0.5216, 0.5216, 0.5216],\n",
       "           ...,\n",
       "           [0.3490, 0.3490, 0.4745,  ..., 0.3882, 0.3922, 0.3529],\n",
       "           [0.2863, 0.3137, 0.3529,  ..., 0.3765, 0.3529, 0.3765],\n",
       "           [0.3255, 0.3412, 0.3412,  ..., 0.4627, 0.4039, 0.3373]],\n",
       " \n",
       "          [[0.2078, 0.3529, 0.3059,  ..., 0.6235, 0.6235, 0.6235],\n",
       "           [0.3020, 0.3020, 0.2980,  ..., 0.6235, 0.6235, 0.6235],\n",
       "           [0.3294, 0.3020, 0.2784,  ..., 0.6235, 0.6235, 0.6235],\n",
       "           ...,\n",
       "           [0.3059, 0.2941, 0.4196,  ..., 0.3451, 0.3490, 0.3020],\n",
       "           [0.2588, 0.2706, 0.3098,  ..., 0.3255, 0.3020, 0.3333],\n",
       "           [0.3020, 0.3059, 0.3059,  ..., 0.4078, 0.3529, 0.3059]]]]),\n",
       " 'extrinsic_0': tensor([[[[-3.8521e-01,  9.2279e-01,  7.7683e-03, -1.0011e+01],\n",
       "           [-2.9342e-02, -3.8339e-03, -9.9956e-01,  1.5071e+00],\n",
       "           [-9.2236e-01, -3.8527e-01,  2.8553e-02,  6.5743e-02],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-6.1904e-01,  7.8533e-01,  5.9529e-03, -3.4098e+01],\n",
       "           [-3.7439e-02, -2.1938e-02, -9.9906e-01,  1.3195e+00],\n",
       "           [-7.8446e-01, -6.1868e-01,  4.2983e-02,  6.1705e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]]]),\n",
       " 'intrinsic_0': tensor([[[310.1505,   0.0000, 315.5628],\n",
       "          [  0.0000, 310.0297, 166.2019],\n",
       "          [  0.0000,   0.0000,   1.0000]],\n",
       " \n",
       "         [[310.1505,   0.0000, 315.5628],\n",
       "          [  0.0000, 310.0297, 166.2019],\n",
       "          [  0.0000,   0.0000,   1.0000]]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_loader = iter(train_dataloader)\n",
    "next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "482ab7e1-0427-418e-a1eb-9fc225c2c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "*** training start ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9243 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 12 but got size 13 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2511/366494375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# 深度を推定する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0minv_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepth_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rgb_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0minv_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0minv_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_depth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 12 but got size 13 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR   \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "# GPUを使う場合（マルチGPUは非対応）\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# num_batch_accumulation = 4\n",
    "\n",
    "depth_net.train().to(device)\n",
    "pose_net.train().to(device)\n",
    "\n",
    "# optimizerを定義。depth_netとpose_netの２つのネットワークのパラメタを渡す。\n",
    "optimizer = Adam([\n",
    "    {\"params\": depth_net.parameters()},\n",
    "    {\"params\": pose_net.parameters()}],\n",
    "    lr=lr,\n",
    "    )\n",
    "\n",
    "# learning rate schecdulerを定義。徐々にlrを減衰させる。\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.5)\n",
    "\n",
    "print(\"*** training start ***\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    with tqdm(train_dataloader) as pbar:\n",
    "        for j, batch in enumerate(pbar):\n",
    "            # GPUにバッチを転送する\n",
    "            batch = {k:v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # 深度を推定する\n",
    "            inv_depth = depth_net(batch[\"rgb_0\"])\n",
    "            inv_depth = F.relu(inv_depth)\n",
    "            inv_depth = inv_depth.squeeze(1)\n",
    "            inv_depth = (inv_depth + 1e-2) / (1 + 1e-2) # inverse depthの最小値（最長深度）を1e-2（100）とする。\n",
    "            depth = 1 / inv_depth\n",
    "\n",
    "            # 姿勢を推定する\n",
    "            image_concat = torch.cat([batch[\"rgb_0\"], batch[\"rgb_-1\"]], axis=1) # ソースとターゲットの２枚の画像を同時に入力する\n",
    "            pose = torch.tanh(pose_net(image_concat))\n",
    "            rotation, translation = pose[..., 0:3], pose[..., 3:]\n",
    "            rotation = rotation * math.pi # 各軸の回転を最大でPiに限定する\n",
    "            translation = translation * 5.0 # 各軸の並進を最大で5.0に限定する\n",
    "            rtmat = tgm.rtvec_to_pose(torch.cat([rotation, translation], dim=-1))\n",
    "\n",
    "            # Lossを計算する\n",
    "            y = {k:v for k, v in batch.items() if k.startswith(\"rgb_\") or k.startswith(\"intrinsic_\")}\n",
    "            y_pred = {\n",
    "                \"depth_0\": depth,\n",
    "                \"inv_depth_0\": inv_depth,\n",
    "                \"extrinsic_-1\": rtmat,\n",
    "            }\n",
    "            loss_ph, image_warped, automask = photometric_loss(y, y_pred)\n",
    "            loss_sm = smoothness_loss(y, y_pred)\n",
    "            loss = (loss_ph * 0.95) + (loss_sm * 0.05)\n",
    "            loss.backward()\n",
    "\n",
    "            # if (j + 1) % num_batch_accumulation == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if j % 200 == 0:\n",
    "                # デバッグのために１枚分の出力を表示する\n",
    "                plt.figure(figsize=(20, 20))\n",
    "                ax_image = plt.subplot(4, 1, 1)\n",
    "                ax_warped = plt.subplot(4, 1, 2)\n",
    "                ax_depth = plt.subplot(4, 1, 3)\n",
    "                ax_automask = plt.subplot(4, 1, 4)\n",
    "\n",
    "                ax_image.set_title(\"target image\")\n",
    "                ax_image.imshow(batch[\"rgb_0\"][0].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "                \n",
    "                ax_warped.set_title(\"warped image (source to target)\")\n",
    "                ax_warped.imshow(image_warped[-1][0].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "\n",
    "                ax_depth.set_title(\"inverse depth map\")\n",
    "                ax_depth.imshow(inv_depth[0].detach().cpu().numpy())\n",
    "\n",
    "                ax_automask.set_title(\"auto-masking (if disabled, it's filled by zeros)\")\n",
    "                ax_automask.imshow(automask[0].detach().cpu().numpy())\n",
    "                \n",
    "                os.makedirs(\"debug\", exist_ok=True)\n",
    "                plt.savefig(f\"debug/epoch_{i}_iter_{j}_output.jpeg\")\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "            # プログレスバーに現在の状態を出力する\n",
    "            pbar.set_description(\n",
    "                f\"[Epoch {i}] loss (ph): {loss_ph:0.3f}, \" \\\n",
    "                f\"loss (sm) {loss_sm:0.3f}, \" \\\n",
    "                f\"depth mean {depth.mean():0.3f}, \" \\\n",
    "                f\"lr {scheduler.get_last_lr()[0]:0.6f}, \" \\\n",
    "                f\"trans mag {torch.linalg.vector_norm(pose[..., 3:], ord=2, dim=-1).mean():0.3f}\")\n",
    "\n",
    "    os.makedirs(\"../ckpt/\", exist_ok=True)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": {\n",
    "                \"depth_net\": depth_net.state_dict(),\n",
    "                \"pose_net\": pose_net.state_dict(),\n",
    "            }\n",
    "        }, f\"../ckpt/models_{i}_epoch.pt\"    \n",
    "    )\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186a8c1-f7ab-45cc-8fc8-8086d4e58e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d3df6-2329-4c43-a8da-d9a5e9c3b9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
