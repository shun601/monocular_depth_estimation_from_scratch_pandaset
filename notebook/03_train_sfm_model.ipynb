{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cee9fa6-0b22-47db-b970-22ae3b217b86",
   "metadata": {},
   "source": [
    "# 単眼深度推定モデルを学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc728c2-a249-4b7f-99f3-4966fce0c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1beef-ffe6-443b-8bef-8a1ad5935cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed204ba7-d45f-45f6-aa64-e13bdbf9d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_inds = [0, -1] # 隣接フレームの番号\n",
    "epochs = 1 #100\n",
    "lr = 0.0004\n",
    "batch_size = 2 #32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33003d41-9f92-4460-8629-f0a721f67be6",
   "metadata": {},
   "source": [
    "# ネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdd6ee-5b6f-484e-b1a7-330485bafbfc",
   "metadata": {},
   "source": [
    "学習するネットワークは現在のフレームの深度マップを推定するものと、フレーム間の姿勢の変化を推定するものの２つである。  \n",
    "ここではそれぞれdepth netとpose netと呼ぶ。\n",
    "- depth netは深度マップを推定するネットワーク\n",
    "- pose netは回転（X軸、Y軸、Z軸）と並進（X, Y, Z）の合計6個の数値を推定するネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9e2f1-665b-4277-9cbd-b2aad917b78d",
   "metadata": {},
   "source": [
    "## depth net\n",
    "ここではネットワークの詳細に関心はないたえ、`segmentation_models_pytorch`を使いU-Netを定義する。  \n",
    "最新の深度推定モデルは複数の解像度の深度マップを推定するのが一般的だが、ここではGPUのメモリを節約するために、１枚のみとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8bbf17-f196-4464-b4dd-59113267f8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6280, -0.5614,  1.3001,  ...,  2.6420, -0.6753,  0.4146],\n",
       "          [ 0.8181, -0.8600, -2.7172,  ..., -3.6206, -1.1865, -1.6223],\n",
       "          [-1.6819, -4.0699, -4.3397,  ..., -5.5163, -4.3460, -2.8407],\n",
       "          ...,\n",
       "          [-1.6891,  0.6863, -0.9573,  ..., -2.3856, -3.5506, -2.7069],\n",
       "          [-3.0193,  1.0049, -0.3561,  ..., -2.5761, -2.1179, -2.3346],\n",
       "          [-4.6208, -0.6131, -1.5683,  ..., -0.7278, -0.0665,  0.6055]]],\n",
       "\n",
       "\n",
       "        [[[-0.6200, -0.5838,  1.2967,  ...,  2.5919, -0.6405,  0.4058],\n",
       "          [ 0.8455, -0.8227, -2.7305,  ..., -3.5190, -1.0787, -1.5844],\n",
       "          [-1.6813, -4.1002, -4.3449,  ..., -5.3299, -4.2327, -2.7842],\n",
       "          ...,\n",
       "          [-1.7251,  0.6954, -0.9701,  ..., -2.3549, -3.5473, -2.6910],\n",
       "          [-3.0105,  1.0248, -0.3233,  ..., -2.6120, -2.1387, -2.3369],\n",
       "          [-4.6372, -0.6355, -1.5522,  ..., -0.7662, -0.0890,  0.6005]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "depth_net = smp.Unet(\"efficientnet-b0\", in_channels=3, classes=1, activation=None)\n",
    "\n",
    "# 入出力確認\n",
    "depth_net(torch.zeros(2, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602f629-eb17-4f27-bc0c-a73b8ba4c002",
   "metadata": {},
   "source": [
    "## pose net\n",
    "pose netに関してもネットワークの詳細に関心がないため、`torchvision`の学習済みモデルを持ってきて、部分的にレイヤーを差し替える。\n",
    "現在フレームと隣接フレームの２枚を入力し、その２枚の間で発生した姿勢の変化量を推定する。  \n",
    "- 入力: 2枚の画像はチャンネルの次元で結合して合計で6チャンネルのテンソルとして入力を行う。\n",
    "- 出力： 出力は並進(X軸、Y軸、Z軸)と回転(X軸、Y軸、Z軸)の6DoFである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b456a6-fb3c-4a42-9f4a-268694766544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8854e-05, -2.9708e-04,  8.1839e-05,  2.7287e-04, -2.1824e-04,\n",
       "         -8.4735e-05]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch.nn import Linear, Conv2d\n",
    "\n",
    "pose_net = resnet18(pretrained=True)\n",
    "pose_net.conv1 = Conv2d(in_channels=3*2, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "pose_net.fc = Linear(in_features=512, out_features=6)\n",
    "\n",
    "# 初期はすべて0を出力したほうが都合がよいので、weightとbiasを0クリアしておく\n",
    "# reproj_loss_ += torch.randn(reproj_loss_.shape).to(device=reproj_loss_.device) * 1e-3\n",
    "pose_net.fc.weight.data = torch.randn(pose_net.fc.weight.shape) * 1e-5\n",
    "pose_net.fc.bias.data = torch.randn(pose_net.fc.bias.shape) * 1e-5\n",
    "\n",
    "# 入出力確認\n",
    "pose_net(torch.zeros(1, 3*2, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91511f1-ef0d-4e4c-9780-0cf7e31d8fe8",
   "metadata": {},
   "source": [
    "# Loss関数\n",
    "ここでは、Phtometric lossとSmoothness lossを定義する。  \n",
    "Photometric lossは推定した深度と姿勢変化からSourceの画像をTargetの画像に合わせて一致しているかどうかをl1とSSIMで測る。  \n",
    "また、[Monodepth2](https://arxiv.org/abs/1806.01260)で提案されたOcculusionや動体から生じる原理的に復元不可能な画素に対するLossの計算を回避するauto-maskingを導入した。  \n",
    "Smoothness lossはPhotometric lossの復元誤差が濃淡が平滑な領域で勾配を得にくいという問題を解決するために導入した。  \n",
    "このlossは近接ピクセルがおおよそ同じような深度を持っている（物体の境界以外は）という仮定のもと、それをLossとして与えるものである。  \n",
    "実装は様々あるが、ここではX,Y方向の差分のみから計算する比較的にシンプルなものを用いる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ea5fd3-5bd7-4897-9ce2-6e9c833af4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d01700d-7ee7-4d90-a914-bb25801e597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from camera import PinholeCamera\n",
    "from functools import lru_cache\n",
    "import torchgeometry as tgm\n",
    "\n",
    "\n",
    "class PhotometricLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, frame_inds, weights=[0.6, 0.4], automasking=False):\n",
    "        \"\"\"\n",
    "        Photometric loss\n",
    "        weights: l1とssimのlossに対する重みを指定する。デフォルトは[0.4, 0.6]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.frame_inds = frame_inds\n",
    "        self.l1_loss = torch.nn.L1Loss(reduction=\"none\")\n",
    "        self.ssim_loss = tgm.losses.SSIM(reduction='none', window_size=3)\n",
    "        self.weights = weights\n",
    "        self.automasking = automasking\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        image_target = y[\"rgb_0\"]\n",
    "        depth = y_pred[f\"depth_0\"]\n",
    "        intrinsic = y[\"intrinsic_0\"]\n",
    "\n",
    "        reproj_loss = []\n",
    "        image_warped = {}\n",
    "        for idx in self.frame_inds:\n",
    "            if idx == 0: # ターゲット同士は比較しない\n",
    "                continue\n",
    "            image_source = y[f\"rgb_{idx}\"]\n",
    "            extrinsic_src2tgt = y_pred[f\"extrinsic_{idx}\"]\n",
    "            image_warped_ = self.warp(image_source, depth, intrinsic, extrinsic_src2tgt)\n",
    "            l1_loss = self.l1_loss(image_warped_, image_target)\n",
    "            ssim_loss = self.ssim_loss(image_warped_, image_target)\n",
    "            reproj_loss_ = l1_loss * self.weights[0] + ssim_loss * self.weights[1]\n",
    "            reproj_loss_ = torch.mean(reproj_loss_, dim=1) # auto-maskingで扱いやすいようにチャンネルの次元を潰しておく\n",
    "            reproj_loss.append(reproj_loss_)\n",
    "            image_warped[idx] = image_warped_\n",
    "        \n",
    "        if self.automasking:\n",
    "            # auto-masking (https://arxiv.org/pdf/1806.01260.pdf) 何も変更を加えないSource画像を利用する。\n",
    "            for idx in self.frame_inds:\n",
    "                if idx == 0: # ターゲット同士は比較しない\n",
    "                    continue\n",
    "                image_source = y[f\"rgb_{idx}\"]\n",
    "                l1_loss = self.l1_loss(image_source, image_target)\n",
    "                ssim_loss = self.ssim_loss(image_source, image_target)\n",
    "                reproj_loss_ = l1_loss * self.weights[0] + ssim_loss * self.weights[1]\n",
    "                # 平坦な領域ではWarpされたものと何も変更を加えないものでLossが全くおなじになってしまう画素が生じる可能性があるので微小な乱数を加える\n",
    "                reproj_loss_ += torch.randn(reproj_loss_.shape).to(device=reproj_loss_.device) * 1e-3\n",
    "                reproj_loss_ = torch.mean(reproj_loss_, dim=1) # auto-maskingで扱いやすいようにチャンネルの次元を潰しておく\n",
    "                reproj_loss.append(reproj_loss_)\n",
    "\n",
    "            reproj_loss = torch.stack(reproj_loss, dim=1)\n",
    "            loss, min_inds = torch.min(reproj_loss, dim=1)\n",
    "            automask = (min_inds >= (reproj_loss.shape[1] // 2)).float()\n",
    "            loss = reproj_loss.mean()\n",
    "        else:\n",
    "            loss = torch.stack(reproj_loss, dim=1)\n",
    "            automask = torch.zeros_like(loss).squeeze(1) # dummy\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        return loss, image_warped, automask\n",
    "\n",
    "    def warp(self, image_source, depth, intrinsic, extrinsic_src2tgt):        \n",
    "        \"\"\" 推定された深度と姿勢からソースをターゲットに重ね合わせる \"\"\"\n",
    "        image_coords = self.create_image_coords(depth.shape)\n",
    "        image_coords = image_coords.to(depth.device)\n",
    "        world_coords = PinholeCamera.image2world(image_coords, intrinsic, extrinsic_src2tgt, depth, batch=True)        \n",
    "        # これまでノートとは異なりターゲットのカメラへの座標変換が終わっているのでworld2camera()ではなくcamera2image()を呼び出す\n",
    "        image_coords = PinholeCamera.camera2image(world_coords[..., :3], intrinsic, batch=True)\n",
    "        # PyTorchのgrid samplingはcv2.remapとは異なり、座標が[-1, 1]に正規化されたものを入力する\n",
    "        image_coords[..., 0] = image_coords[..., 0] / image_coords.shape[2] * 2 - 1\n",
    "        image_coords[..., 1] = image_coords[..., 1] / image_coords.shape[1] * 2 - 1\n",
    "        grid = image_coords\n",
    "        image_warped = F.grid_sample(image_source, grid, align_corners=False)\n",
    "        return image_warped\n",
    "                \n",
    "    @lru_cache(None)\n",
    "    def create_image_coords(self, map_shape):\n",
    "        \"\"\" 各画素に対する画像座標を生成する \"\"\"\n",
    "        xi = torch.arange(0, map_shape[2], 1)\n",
    "        yi = torch.arange(0, map_shape[1], 1)\n",
    "        coord_x, coord_y = torch.meshgrid(xi, yi, indexing=\"xy\")\n",
    "        image_coords = torch.stack([coord_x, coord_y], axis=-1)\n",
    "        image_coords = image_coords.float()        \n",
    "        image_coords = image_coords.unsqueeze(0).repeat(map_shape[0], 1, 1, 1) # バッチ化\n",
    "        return image_coords\n",
    "\n",
    "\n",
    "class SmoothnessLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        depth = y_pred[f\"inv_depth_0\"]\n",
    "        gradients_y = torch.mean(torch.abs(inv_depth[..., :-1,  :] - inv_depth[..., 1:,  :]))\n",
    "        gradients_x = torch.mean(torch.abs(inv_depth[..., :  ,:-1] - inv_depth[...,  :, 1:]))\n",
    "        return (gradients_x + gradients_y) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3972718-5505-49f7-8981-826e8090b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "photometric_loss = PhotometricLoss(frame_inds=frame_inds)\n",
    "smoothness_loss = SmoothnessLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14ea33-2d55-4013-a18f-97538894aef9",
   "metadata": {},
   "source": [
    "# Pose Utils\n",
    "pose netが推定した姿勢の変化量(6DoF)を4x4の行列に変換する。  \n",
    "`torchgeometry`の`rvec_to_pose`を使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da6b39d-b038-4d49-9a03-b1dcb9a89ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 実行例　6DoFが4x4の行列に変換されることを確認する\n",
    "tgm.rtvec_to_pose(torch.rand(3, 6)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17cfbf-b629-42e9-bb85-c65115e52e64",
   "metadata": {},
   "source": [
    "## training loop\n",
    "ここからようやく学習を実行する。<br>\n",
    "この学習はGTX1060（6GB RAM）で動作することが確認できている。<br>\n",
    "より大きなRAMを搭載したGPUをつかえばバッチ数、フレーム数（frame_inds）を増やし、学習を安定化させることができるはずである。\n",
    "\n",
    "現状は学習の初期が非常に不安定であり、depth meanがnanになってしまうことが多い。<br>\n",
    "学習が安定するまで何度か実行し直す必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db84c088-28a0-4d56-90cb-15136705f6dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sequences\n"
     ]
    }
   ],
   "source": [
    "# 学習用のデータセットクラスの読み込みを行う\n",
    "from dataset import Pandaset\n",
    "train_dataset = Pandaset(root_dir=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0c953fe-8bed-4173-8b56-4ae89c24cffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"rgb_0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a0575f7-47e8-44e6-81e7-b1212f5e4a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[933.4667,   0.    , 896.4692],\n",
       "       [  0.    , 934.6754, 507.3557],\n",
       "       [  0.    ,   0.    ,   1.    ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"intrinsic_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf53361-5961-4031-bd18-b87f538ec37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Transform(Dataset):\n",
    "    \"\"\"\n",
    "    ネットワークへの入力に適切な形に変換するクラス。\n",
    "    主に画像のリサイズとそれに伴う内部パラメタの補正を行う。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):        \n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.crop = (45, 43, 700, 331) # (x0, y0, x1, y1)\n",
    "        self.scale = 1.0 / 3.0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        for key in data.keys():\n",
    "            if key.startswith(\"rgb_\") or key.startswith(\"depth_\"):\n",
    "                # 画像と深度を変換する\n",
    "                image = data[key]\n",
    "                # クロップと縮小。CNNに入力する都合でキリの良い画素数に変更する必要があり、\n",
    "                # ここでは1242x375--crop-->1152x288--resize-->384x96としている。\n",
    "                image = image[self.crop[1]:self.crop[3], self.crop[0]:self.crop[2]]\n",
    "                orig_shape = image.shape\n",
    "                dest_size = (int(orig_shape[1] * self.scale), int(orig_shape[0] * self.scale))\n",
    "                image = cv2.resize(image, dest_size, interpolation=cv2.INTER_LINEAR)\n",
    "                data[key] = torch.tensor(image).float()\n",
    "                if key.startswith(\"rgb_\"):\n",
    "                    data[key] = data[key].permute(2, 0, 1) # (B, H, W, C) -> (B, C, H, W)\n",
    "                    data[key] /= 255.0 # normalize\n",
    "            elif key.startswith(\"intrinsic_\"):\n",
    "                # 画像がリサイズとクロップに合わせて内部パラメタを補正する\n",
    "                intrinsic = data[key]\n",
    "                intrinsic[0, 2] = intrinsic[0, 2] - self.crop[0]\n",
    "                intrinsic[1, 2] = intrinsic[1, 2] - self.crop[1]\n",
    "                intrinsic[:2, :] *= self.scale\n",
    "                data[key] = torch.tensor(intrinsic).float()\n",
    "            else:\n",
    "                data[key] = torch.tensor(data[key]).float()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(Transform(train_dataset), batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e08db7e-1d49-45c0-8f9d-43faefb2010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(train_dataloader)\n",
    "batch_ = next(iter_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75d5099b-2e1d-4e44-8815-125a857daae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 96, 218])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_[\"rgb_-1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1075b18d-acdb-4cb9-90b7-d7aec2f8fdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 96, 218])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_[\"rgb_0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2db0af33-52f0-42a7-8577-dbf9596e0a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rgb_-1', 'extrinsic_-1', 'intrinsic_-1', 'rgb_0', 'extrinsic_0', 'intrinsic_0'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0770b893-aa27-43a6-bcec-4583ae3e9cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_[\"extrinsic_-1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc63bc9b-2bfa-4633-ae1d-d60bab880a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_[\"intrinsic_-1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e15956-fe3e-4049-ade9-71d413b3bd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.zeros(2, 3, 224, 224)\n",
    "print(dummy_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "841e29b5-8793-4897-8790-6bb4e58e9a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6275, -0.5732,  1.2998,  ...,  2.6235, -0.6627,  0.4128],\n",
       "          [ 0.8062, -0.8797, -2.8109,  ..., -3.5937, -1.1699, -1.6058],\n",
       "          [-1.6707, -4.0823, -4.3859,  ..., -5.4752, -4.3019, -2.7777],\n",
       "          ...,\n",
       "          [-1.7320,  0.6624, -0.9791,  ..., -2.3569, -3.5475, -2.7057],\n",
       "          [-3.0264,  0.9847, -0.3594,  ..., -2.6423, -2.1570, -2.3310],\n",
       "          [-4.6595, -0.6655, -1.5820,  ..., -0.7443, -0.1104,  0.5976]]],\n",
       "\n",
       "\n",
       "        [[[-0.6249, -0.5656,  1.2809,  ...,  2.6585, -0.7047,  0.4072],\n",
       "          [ 0.8037, -0.9123, -2.8020,  ..., -3.6275, -1.2133, -1.6510],\n",
       "          [-1.6755, -4.1283, -4.3559,  ..., -5.5346, -4.3398, -2.8270],\n",
       "          ...,\n",
       "          [-1.6594,  0.7248, -0.9328,  ..., -2.4181, -3.5658, -2.7401],\n",
       "          [-3.0227,  1.0711, -0.2867,  ..., -2.5838, -2.1239, -2.3326],\n",
       "          [-4.6039, -0.5390, -1.5094,  ..., -0.7166, -0.0789,  0.5741]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_net(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f185acec-c26c-4d7a-9022-4d208a072d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 96, 218])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.zeros(2, 3, 96, 218)\n",
    "print(dummy_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc52099e-dbab-4979-8d06-5fc5329ba400",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 12 but got size 13 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2770/2449175026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdepth_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 12 but got size 13 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "depth_net(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed76cab-f8c1-4812-84ad-dd913242f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_net(batch_[\"rgb_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "482ab7e1-0427-418e-a1eb-9fc225c2c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "*** training start ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9243 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 96, 218])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9243 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 12 but got size 13 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2511/2013459285.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# 深度を推定する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0minv_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepth_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rgb_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0minv_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0minv_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_depth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 12 but got size 13 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR   \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "# GPUを使う場合（マルチGPUは非対応）\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# num_batch_accumulation = 4\n",
    "\n",
    "depth_net.train().to(device)\n",
    "pose_net.train().to(device)\n",
    "\n",
    "# optimizerを定義。depth_netとpose_netの２つのネットワークのパラメタを渡す。\n",
    "optimizer = Adam([\n",
    "    {\"params\": depth_net.parameters()},\n",
    "    {\"params\": pose_net.parameters()}],\n",
    "    lr=lr,\n",
    "    )\n",
    "\n",
    "# learning rate schecdulerを定義。徐々にlrを減衰させる。\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.5)\n",
    "\n",
    "print(\"*** training start ***\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    with tqdm(train_dataloader) as pbar:\n",
    "        for j, batch in enumerate(pbar):\n",
    "            # GPUにバッチを転送する\n",
    "            batch = {k:v.to(device) for k, v in batch.items()}\n",
    "            print(batch[\"rgb_0\"].shape)\n",
    "\n",
    "            # 深度を推定する\n",
    "            inv_depth = depth_net(batch[\"rgb_0\"])\n",
    "            inv_depth = F.relu(inv_depth)\n",
    "            inv_depth = inv_depth.squeeze(1)\n",
    "            inv_depth = (inv_depth + 1e-2) / (1 + 1e-2) # inverse depthの最小値（最長深度）を1e-2（100）とする。\n",
    "            depth = 1 / inv_depth\n",
    "\n",
    "            # 姿勢を推定する\n",
    "            image_concat = torch.cat([batch[\"rgb_0\"], batch[\"rgb_-1\"]], axis=1) # ソースとターゲットの２枚の画像を同時に入力する\n",
    "            pose = torch.tanh(pose_net(image_concat))\n",
    "            rotation, translation = pose[..., 0:3], pose[..., 3:]\n",
    "            rotation = rotation * math.pi # 各軸の回転を最大でPiに限定する\n",
    "            translation = translation * 5.0 # 各軸の並進を最大で5.0に限定する\n",
    "            rtmat = tgm.rtvec_to_pose(torch.cat([rotation, translation], dim=-1))\n",
    "\n",
    "            # Lossを計算する\n",
    "            y = {k:v for k, v in batch.items() if k.startswith(\"rgb_\") or k.startswith(\"intrinsic_\")}\n",
    "            y_pred = {\n",
    "                \"depth_0\": depth,\n",
    "                \"inv_depth_0\": inv_depth,\n",
    "                \"extrinsic_-1\": rtmat,\n",
    "            }\n",
    "            loss_ph, image_warped, automask = photometric_loss(y, y_pred)\n",
    "            loss_sm = smoothness_loss(y, y_pred)\n",
    "            loss = (loss_ph * 0.95) + (loss_sm * 0.05)\n",
    "            loss.backward()\n",
    "\n",
    "            # if (j + 1) % num_batch_accumulation == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if j % 200 == 0:\n",
    "                # デバッグのために１枚分の出力を表示する\n",
    "                plt.figure(figsize=(20, 20))\n",
    "                ax_image = plt.subplot(4, 1, 1)\n",
    "                ax_warped = plt.subplot(4, 1, 2)\n",
    "                ax_depth = plt.subplot(4, 1, 3)\n",
    "                ax_automask = plt.subplot(4, 1, 4)\n",
    "\n",
    "                ax_image.set_title(\"target image\")\n",
    "                ax_image.imshow(batch[\"rgb_0\"][0].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "                \n",
    "                ax_warped.set_title(\"warped image (source to target)\")\n",
    "                ax_warped.imshow(image_warped[-1][0].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "\n",
    "                ax_depth.set_title(\"inverse depth map\")\n",
    "                ax_depth.imshow(inv_depth[0].detach().cpu().numpy())\n",
    "\n",
    "                ax_automask.set_title(\"auto-masking (if disabled, it's filled by zeros)\")\n",
    "                ax_automask.imshow(automask[0].detach().cpu().numpy())\n",
    "                \n",
    "                os.makedirs(\"debug\", exist_ok=True)\n",
    "                plt.savefig(f\"debug/epoch_{i}_iter_{j}_output.jpeg\")\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "            # プログレスバーに現在の状態を出力する\n",
    "            pbar.set_description(\n",
    "                f\"[Epoch {i}] loss (ph): {loss_ph:0.3f}, \" \\\n",
    "                f\"loss (sm) {loss_sm:0.3f}, \" \\\n",
    "                f\"depth mean {depth.mean():0.3f}, \" \\\n",
    "                f\"lr {scheduler.get_last_lr()[0]:0.6f}, \" \\\n",
    "                f\"trans mag {torch.linalg.vector_norm(pose[..., 3:], ord=2, dim=-1).mean():0.3f}\")\n",
    "\n",
    "    os.makedirs(\"../ckpt/\", exist_ok=True)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": {\n",
    "                \"depth_net\": depth_net.state_dict(),\n",
    "                \"pose_net\": pose_net.state_dict(),\n",
    "            }\n",
    "        }, f\"../ckpt/models_{i}_epoch.pt\"    \n",
    "    )\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186a8c1-f7ab-45cc-8fc8-8086d4e58e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d3df6-2329-4c43-a8da-d9a5e9c3b9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
