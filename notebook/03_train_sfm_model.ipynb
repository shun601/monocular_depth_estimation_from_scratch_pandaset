{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cee9fa6-0b22-47db-b970-22ae3b217b86",
   "metadata": {},
   "source": [
    "# 単眼深度推定モデルを学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1beef-ffe6-443b-8bef-8a1ad5935cab",
   "metadata": {},
   "source": [
    "# ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed204ba7-d45f-45f6-aa64-e13bdbf9d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = [0, -1] # 隣接フレームの番号\n",
    "epochs = 100\n",
    "lr = 0.0004\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33003d41-9f92-4460-8629-f0a721f67be6",
   "metadata": {},
   "source": [
    "# ネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdd6ee-5b6f-484e-b1a7-330485bafbfc",
   "metadata": {},
   "source": [
    "学習するネットワークは現在のフレームの深度マップを推定するものと、フレーム間の姿勢の変化を推定するものの２つである。  \n",
    "ここではそれぞれdepth netとpose netと呼ぶ。\n",
    "- depth netは深度マップを推定するネットワーク\n",
    "- pose netは回転（X軸、Y軸、Z軸）と並進（X, Y, Z）の合計6個の数値を推定するネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9e2f1-665b-4277-9cbd-b2aad917b78d",
   "metadata": {},
   "source": [
    "## depth net\n",
    "ここではネットワークの詳細に関心はないたえ、`segmentation_models_pytorch`を使いU-Netを定義する。  \n",
    "最新の深度推定モデルは複数の解像度の深度マップを推定するのが一般的だが、ここではGPUのメモリを節約するために、１枚のみとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8bbf17-f196-4464-b4dd-59113267f8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5409584d684bcd9a4c78ecf6bbe17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/20.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -1.5092,  -3.5802,  -3.9052,  ...,  -5.9807,  -2.9136,  -0.5965],\n",
       "          [ -2.2135,  -1.7661,  -1.1237,  ..., -13.6258,  -5.4054,  -0.3367],\n",
       "          [ -2.0020,  -3.0601,  -2.6086,  ...,  -9.8803, -11.2230,  -5.5673],\n",
       "          ...,\n",
       "          [ -4.6031,  -4.3523,  -0.6034,  ...,   1.9895,   0.9651,  -1.8151],\n",
       "          [ -4.6992,  -3.0708,   0.4737,  ...,   2.7389,  -2.0580,  -0.3783],\n",
       "          [ -0.7423,  -1.1441,   2.3566,  ...,  -2.5374,  -1.5815,  -0.4001]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "depth_net = smp.Unet(\"efficientnet-b0\", in_channels=3, classes=1, activation=None)\n",
    "\n",
    "# 入出力確認\n",
    "depth_net(torch.zeros(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602f629-eb17-4f27-bc0c-a73b8ba4c002",
   "metadata": {},
   "source": [
    "## pose net\n",
    "pose netに関してもネットワークの詳細に関心がないため、`torchvision`の学習済みモデルを持ってきて、部分的にレイヤーを差し替える。\n",
    "現在フレームと隣接フレームの２枚を入力し、その２枚の間で発生した姿勢の変化量を推定する。  \n",
    "- 入力: 2枚の画像はチャンネルの次元で結合して合計で6チャンネルのテンソルとして入力を行う。\n",
    "- 出力： 出力は並進(X軸、Y軸、Z軸)と回転(X軸、Y軸、Z軸)の6DoFである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b456a6-fb3c-4a42-9f4a-268694766544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3985e6f70c144e3596a5c626dadd04f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0028e-04, -3.8320e-04, -4.6174e-05, -9.7556e-07, -1.4228e-04,\n",
       "         -4.3509e-04]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch.nn import Linear, Conv2d\n",
    "\n",
    "pose_net = resnet18(pretrained=True)\n",
    "pose_net.conv1 = Conv2d(in_channels=3*2, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "pose_net.fc = Linear(in_features=512, out_features=6)\n",
    "\n",
    "# 初期はすべて0を出力したほうが都合がよいので、weightとbiasを0クリアしておく\n",
    "# reproj_loss_ += torch.randn(reproj_loss_.shape).to(device=reproj_loss_.device) * 1e-3\n",
    "pose_net.fc.weight.data = torch.randn(pose_net.fc.weight.shape) * 1e-5\n",
    "pose_net.fc.bias.data = torch.randn(pose_net.fc.bias.shape) * 1e-5\n",
    "\n",
    "# 入出力確認\n",
    "pose_net(torch.zeros(1, 3*2, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91511f1-ef0d-4e4c-9780-0cf7e31d8fe8",
   "metadata": {},
   "source": [
    "# Loss関数\n",
    "ここでは、Phtometric lossとSmoothness lossを定義する。  \n",
    "Photometric lossは推定した深度と姿勢変化からSourceの画像をTargetの画像に合わせて一致しているかどうかをl1とSSIMで測る。  \n",
    "また、[Monodepth2](https://arxiv.org/abs/1806.01260)で提案されたOcculusionや動体から生じる原理的に復元不可能な画素に対するLossの計算を回避するauto-maskingを導入した。  \n",
    "Smoothness lossはPhotometric lossの復元誤差が濃淡が平滑な領域で勾配を得にくいという問題を解決するために導入した。  \n",
    "このlossは近接ピクセルがおおよそ同じような深度を持っている（物体の境界以外は）という仮定のもと、それをLossとして与えるものである。  \n",
    "実装は様々あるが、ここではX,Y方向の差分のみから計算する比較的にシンプルなものを用いる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ea5fd3-5bd7-4897-9ce2-6e9c833af4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d01700d-7ee7-4d90-a914-bb25801e597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# from camera import PinholeCamera\n",
    "from functools import lru_cache\n",
    "import torchgeometry as tgm\n",
    "\n",
    "\n",
    "class PhotometricLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, frame_inds, weights=[0.6, 0.4], automasking=False):\n",
    "        super().__init__()\n",
    "        self.frame_inds = frame_inds\n",
    "        self.l1_loss = torch.nn.L1loss(reduction=\"none\")\n",
    "        self.ssim_loss = tgm.losses.SSIM(reduction=\"none\", window_size=3)\n",
    "        self.weights = weights\n",
    "        self.automasking = automasking\n",
    "        \n",
    "    def forward(self, y, y_pred):\n",
    "        image_target = y[\"rgb_0\"]\n",
    "        depth = y_pred[\"depth_0\"]\n",
    "        intrinsic = y[\"intrinsic_0\"]\n",
    "        \n",
    "        reproj_loss = []\n",
    "        image_warped = {}\n",
    "        for idx in self.frame_inds:\n",
    "            if idx == 0: # ターゲット同士は比較しない\n",
    "                continue\n",
    "            image_source = y[\"rgb_{idx}\"]\n",
    "            extrinsic_src2tgt = y_pred[\"extrinsic_{idx}\"]\n",
    "            image_warped_ = self.warp(image_source, depth, intrinsic, extrinsic_src2tgt)\n",
    "            l1_loss = self.l1_loss(image_warped_, image_target)\n",
    "            ssim_loss = self.ssim_loss(image_warped_, image_target)\n",
    "            reproj_loss = l1_loss * self.weights[0] + ssim_loss * self.weights[1]\n",
    "            reproj_loss = torch.mean(reproj_loss_, dim=1) # auto-maskingで扱いやすいようにチャンネルの次元を潰しておく\n",
    "            reproj_loss.append(reproj_loss_)\n",
    "            image_warped[idx] = image_warped_\n",
    "            \n",
    "        if self.automasking:\n",
    "            # auto-masking (https://arxiv.org/pdf/1806.01260.pdf) 何も変更を加えないSource画像を利用する。\n",
    "            for idx in self.frame_inds:\n",
    "                if idx == 0: # ターゲット同士は比較しない\n",
    "                    continue\n",
    "                image_source = y[\"rgb_{idx}\"]\n",
    "                l1_loss = self.l1_loss(image_source, image_target)\n",
    "                ssim_loss = self.ssim_loss(image_source, image_target)\n",
    "                reproj_loss_ = l1_loss * self.weights[0] + ssim_loss * self.weights[1]\n",
    "                # 平坦な領域ではWarpされたものと何も変更を加えないものでLossが全くおなじになってしまう画素が生じる可能性があるので微小な乱数を加える\n",
    "                reproj_loss_ += torch.randn(reproj_loss_.shape).to(device=reproj_loss_.device) * 1e-3\n",
    "                reproj_loss_ = torch.mean(reproj_loss_, dim=1) # auto-maskingで扱いやすいようにチャンネルの次元を潰しておく\n",
    "                reproj_loss.append(reproj_loss_)\n",
    "                \n",
    "            reproj_loss = torch.stack(reproj_loss, dim=1)\n",
    "            loss, min_inds = torch.min(reproj_loss, dim=1)\n",
    "            automask = (min_inds >= (reproj_loss.shape[1] // 2)).float()\n",
    "            loss = reproj_loss.mean()\n",
    "        else:\n",
    "            loss = torch.stack(reproj_loss, dim=1)\n",
    "            automask = torch.zeros_like(loss).squeeze(1) # dummy\n",
    "            loss = loss.mean()\n",
    "            \n",
    "        return loss, image_warped, automask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3972718-5505-49f7-8981-826e8090b781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
